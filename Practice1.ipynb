{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed92e714-58f9-4279-a76a-7eb816dae987",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/31 20:09:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc=SparkContext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "445cb1aa-8430-4bb8-aee4-0e439b427222",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sarikas-air.lan:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43d1e39d-7261-4097-810d-35119d714ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "s=SparkSession.builder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0abf7087-c76f-4ce9-9377-4512f8208c9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.session.SparkSession.Builder at 0x116504c90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9d44dd2-6fc3-487e-8d86-56df94d3820e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/03/31 22:15:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/03/31 22:15:11 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sarikas-air.lan:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "sc=SparkContext()\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e8a3e0f-8e95-42a7-b718-43c54f2c3791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (5, 6)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=sc.parallelize([(1,2),(3,4),(5,6)])\n",
    "x.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f33c3fe-c603-4ce3-9cb7-eb9174806c75",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/02 18:24:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/04/02 18:24:49 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "x=SparkConf().setAppName(\"First Spark APplication\").setMaster(\"local\")\n",
    "sc=SparkContext(conf=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27934229-4271-4db6-b925-bbc8147228ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34f40d15-ad20-4adc-94ba-71c100b9291b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://sarikas-air.lan:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>First Spark APplication</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local appName=First Spark APplication>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d78e6516-d7ab-4494-88ed-e763aceb7689",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 2), (3, 4), (5, 6), (7, 8), (9, 10)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=sc.parallelize([(1,2),(3,4),(5,6),(7,8),(9,10)])\n",
    "x.collect()                 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3893ad22-f821-47c5-bb48-4f1ea0c40757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext=SQLContext(sc)\n",
    "x=sc.parallelize([(1,2),(3,4),(5,6),(7,8),(9,10)]).toDF(['col1','col2'])\n",
    "x.collect()\n",
    "x.first()\n",
    "x.count()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c6cce886-125a-4f3f-8d16-a5aef8c4180e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff37ee4e-8d45-4d77-a828-6a6c60469cdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(col1=1, col2=2)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "663e2b03-c49c-4e24-b9d7-3cbf415c0b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba9b3531-d2c1-4b61-82e5-5bafeaa2d8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.getOrCreate()\n",
    "sc=spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "05a97f27-a69a-4e6a-894d-6b2210dfdce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile=sc.textFile(\"/Users/sarikakondur/Documents/reqirements.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bc6c408-d1fc-4f89-9759-aff0dc5655d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sklearn==0.17.1', 'pyarrow==6.0.1']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textfile.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "852ae1d1-9172-4f5a-9b2c-c0c68c7420af",
   "metadata": {},
   "outputs": [],
   "source": [
    "textfile.saveAsTextFile(\"/Users/sarikakondur/Documents/reqirementsNew.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "08bded36-d568-4ece-a244-c1ec7c2362ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sklearn==0.17.1'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textfile.first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "205c96c1-76b2-403c-9d3e-c522d95d2e28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sklearn==0.17.1', 'pyarrow==6.0.1']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textfile.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "33911e74-ac16-4500-bce8-a5a5feb7eca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lineswithSpark=textfile.filter(lambda x: \"sklearn\" in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dab5e93c-7a89-43f9-afdb-3c88830c878e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sklearn==0.17.1']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineswithSpark.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1aaeae5-7ac8-4a75-90c2-43192bc4620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a comment for testing git."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
